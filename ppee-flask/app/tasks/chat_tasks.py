"""–ó–∞–¥–∞—á–∏ Celery –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —á–∞—Ç–∞ —Å –ø–æ–º–æ—â–Ω–∏–∫–∞–º–∏"""

from app import celery, db, create_app
from app.models import Application, Message, Conversation, Checklist
from app.services.fastapi_client import FastAPIClient
import logging
import requests
import time
from datetime import datetime

logger = logging.getLogger(__name__)


def process_chat_message_impl(conversation_id, message_id, template_id=None):
    """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏—è (—è–¥—Ä–æ –ª–æ–≥–∏–∫–∏ –¥–ª—è —á–∞—Ç–∞)."""
    app = create_app()
    with app.app_context():
        # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        conversation = Conversation.query.get(conversation_id)
        message = Message.query.get(message_id)
        if not conversation or not message:
            return {'status': 'error', 'message': '–î–∏–∞–ª–æ–≥ –∏–ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã'}
        assistant = conversation.assistant
        if not assistant:
            return {'status': 'error', 'message': '–ü–æ–º–æ—â–Ω–∏–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω'}

        fastapi_url = app.config.get('FASTAPI_URL', 'http://localhost:8002')
        logger.info(f"[CHAT] FASTAPI_URL={fastapi_url}")

        # –ü–æ–∏—Å–∫
        search_results = []
        if assistant.enable_search and assistant.status in ['indexed', 'analyzed']:
            try:
                logger.info(f"[CHAT] POST {fastapi_url}/search application_id={assistant.id} query='{message.content[:50]}'")
                search_response = requests.post(
                    f"{fastapi_url}/search",
                    json={
                    "application_id": str(assistant.id),
                    "query": message.content,
                    "limit": assistant.search_limit or 10,
                    "use_reranker": assistant.use_reranker,
                    "rerank_limit": 20 if assistant.use_reranker else None,
                    "use_smart_search": True,
                    "vector_weight": 0.5,
                    "text_weight": 0.5,
                        "hybrid_threshold": 10
                    },
                    timeout=10
                )
                if search_response.status_code == 200:
                    data = search_response.json()
                    search_results = data.get("results", [])
                    sample_text = ''
                    if search_results:
                        sample_text = (search_results[0].get('text') or '')[:120].replace('\n', ' ')
                    logger.info(f"[CHAT] /search resp status=200 results={len(search_results)} sample='{sample_text}'")
                else:
                    body = (search_response.text or '')[:200].replace('\n', ' ')
                    logger.warning(f"[CHAT] /search resp status={search_response.status_code} body='{body}'")
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ: {e}")

        # –®–∞–±–ª–æ–Ω
        prompt_template = None
        # 1) –Ø–≤–Ω–æ –≤—ã–±—Ä–∞–Ω–Ω—ã–π —à–∞–±–ª–æ–Ω –∏–∑ UI
        if template_id:
            template = Checklist.query.get(template_id)
            prompt_template = template.get_prompt_template() if template else None
        # 2) –ò–Ω–∞—á–µ ‚Äî –ø–µ—Ä–≤–∞—è –ø—Ä–∏–≤—è–∑–∞–Ω–Ω–∞—è –∫ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç—É –∑–∞–≥–æ—Ç–æ–≤–∫–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å)
        if not prompt_template and assistant.checklists:
            try:
                first_template = assistant.checklists[0] if isinstance(assistant.checklists, list) else assistant.checklists.first()
                if first_template:
                    prompt_template = first_template.get_prompt_template()
            except Exception:
                pass
        # 3) –§–æ–ª–ª–±–µ–∫ –ø–æ —Ç–∏–ø—É –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
        if not prompt_template:
            if assistant.assistant_type == 'support':
                prompt_template = """–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç —Å–ª—É–∂–±—ã –ø–æ–¥–¥–µ—Ä–∂–∫–∏. –ù–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ–º–æ–≥–∏ —Ä–µ—à–∏—Ç—å –ø—Ä–æ–±–ª–µ–º—É –∫–ª–∏–µ–Ω—Ç–∞.

–ó–∞–ø—Ä–æ—Å –∫–ª–∏–µ–Ω—Ç–∞: {query}

–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π:
{context}

–ü—Ä–µ–¥–æ—Å—Ç–∞–≤—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º–∏ —à–∞–≥–∞–º–∏ —Ä–µ—à–µ–Ω–∏—è."""
            else:
                prompt_template = """–¢—ã - –ø–æ–º–æ—â–Ω–∏–∫. –û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.

–í–æ–ø—Ä–æ—Å: {query}

–î–æ—Å—Ç—É–ø–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:
{context}

–î–∞–π –ø–æ–ª–µ–∑–Ω—ã–π –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π –æ—Ç–≤–µ—Ç."""

        context = format_search_context(search_results)
        history_context = conversation.get_context_for_llm(limit=5)
        if history_context:
            context = f"–ò—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞:\n{history_context}\n\n" + context

        final_prompt = prompt_template.format(
            query=message.content,
            context=context if context else "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π."
        )
        system_prompt = assistant.get_system_prompt()
        if system_prompt:
            final_prompt = f"{system_prompt}\n\n{final_prompt}"

        # –í—ã–∑–æ–≤ LLM
        try:
            model_to_use = _resolve_llm_model_for_message(assistant, template_id)
            logger.info(f"[CHAT] POST {fastapi_url}/llm/process model={model_to_use}")
            llm_response = requests.post(
                f"{fastapi_url}/llm/process",
                json={
                # –ï—Å–ª–∏ –≤ –≤—ã–±—Ä–∞–Ω–Ω–æ–º/—Å–≤—è–∑–∞–Ω–Ω–æ–º —à–∞–±–ª–æ–Ω–µ –∑–∞–¥–∞–Ω —Ç–µ–≥ llm:MODEL ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ, –∏–Ω–∞—á–µ –º–æ–¥–µ–ª—å –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
                "model_name": model_to_use,
                "prompt": final_prompt,
                "context": "",
                "parameters": {
                    'temperature': assistant.temperature or 0.7,
                    'max_tokens': assistant.max_tokens or 2000,
                    'search_query': message.content
                },
                    "query": message.content
                },
                timeout=60
            )
            if llm_response.status_code == 200:
                llm_data = llm_response.json()
                response_text = llm_data.get("response", "–ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç")
                tokens_info = llm_data.get("tokens", {})
                # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª–Ω—ã–π —Å—ã—Ä–æ–π –æ—Ç–≤–µ—Ç API –≤ —Å–∫—Ä—ã–≤–∞–µ–º—ã–π –±–ª–æ–∫ –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
                raw = (llm_response.text or '')
                safe_raw = raw.replace('&', '&amp;').replace('<', '&lt;').replace('>', '&gt;')
                response_text = (
                    f"{response_text}\n\n"
                    f"<div class=\"tech-details\" style=\"display:none;margin-top:8px\"><small>API raw:</small>"
                    f"<pre style=\"white-space:pre-wrap\">{safe_raw}</pre></div>"
                )
            else:
                # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–ª–æ –æ—à–∏–±–∫–∏ –≤ —Å–∫—Ä—ã–≤–∞–µ–º—ã–π –±–ª–æ–∫
                body = llm_response.text or ''
                safe_body = body.replace('&', '&amp;').replace('<', '&lt;').replace('>', '&gt;')
                response_text = (
                    "–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑."
                    f"\n\n<div class=\"tech-details\" style=\"display:none;margin-top:8px\"><small>API error {llm_response.status_code}:</small>"
                    f"<pre style=\"white-space:pre-wrap\">{safe_body}</pre></div>"
                )
                tokens_info = {}
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–∑–æ–≤–µ LLM: {e}")
            response_text = "–ò–∑–≤–∏–Ω–∏—Ç–µ, —Å–µ—Ä–≤–∏—Å –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."
            tokens_info = {}

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        assistant_message = Message(
            conversation_id=conversation_id,
            role='assistant',
            content=response_text,
            search_results=search_results[:5] if search_results else None,
            search_query=message.content,
            llm_request={
                'prompt': final_prompt[:1000],
                'model': assistant.llm_model,
                'temperature': assistant.temperature,
                'max_tokens': assistant.max_tokens
            },
            llm_response=tokens_info,
            tokens_count=tokens_info.get('total_tokens', 0),
            processing_time=time.time() - message.created_at.timestamp() if message.created_at else 0
        )
        db.session.add(assistant_message)
        conversation.update_statistics()
        assistant.increment_message_count()
        if template_id:
            template = Checklist.query.get(template_id)
            if template:
                template.increment_usage()
        db.session.commit()

        return {
            'status': 'success',
            'message': response_text,
            'message_id': assistant_message.id,
            'search_results_count': len(search_results),
            'tokens_used': tokens_info.get('total_tokens', 0),
            'processing_time': assistant_message.processing_time
        }


@celery.task(bind=True)
def process_chat_message(self, conversation_id, message_id, template_id=None):
    """Celery-–æ–±–µ—Ä—Ç–∫–∞ —Å –ø—Ä–æ–≥—Ä–µ—Å—Å–æ–º –≤–æ–∫—Ä—É–≥ —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –∏–º–ø–ª–µ–º–µ–Ω—Ç–∞—Ü–∏–∏."""
    try:
        self.update_state(state='PROGRESS', meta={'status': 'progress', 'progress': 5, 'message': '–°—Ç–∞—Ä—Ç...'})
        result = process_chat_message_impl(conversation_id, message_id, template_id)
        if isinstance(result, dict) and result.get('status') == 'success':
            self.update_state(state='SUCCESS', meta={'status': 'success', 'progress': 100, 'message': '–ì–æ—Ç–æ–≤–æ'})
        else:
            self.update_state(state='FAILURE', meta={'status': 'error', 'message': result.get('message') if isinstance(result, dict) else '–û—à–∏–±–∫–∞'})
        return result
    except Exception as e:
        logger.exception(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ process_chat_message: {e}")
        self.update_state(state='FAILURE', meta={'status': 'error', 'message': str(e)})
        return {'status': 'error', 'message': f'–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {str(e)}'}


def format_search_context(search_results, max_results=5):
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ LLM"""
    if not search_results:
        return ""
    
    context_parts = []
    for i, result in enumerate(search_results[:max_results], 1):
        text = result.get('text', '')
        metadata = result.get('metadata', {})
        
        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        part = f"[–î–æ–∫—É–º–µ–Ω—Ç {i}]\n"
        if metadata.get('document_id'):
            part += f"–ò—Å—Ç–æ—á–Ω–∏–∫: {metadata['document_id']}\n"
        if metadata.get('page_number'):
            part += f"–°—Ç—Ä–∞–Ω–∏—Ü–∞: {metadata['page_number']}\n"
        part += f"–¢–µ–∫—Å—Ç: {text[:500]}..."  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É —Ç–µ–∫—Å—Ç–∞
        
        context_parts.append(part)
    
    return "\n\n".join(context_parts)


@celery.task(bind=True)
def process_support_query(self, assistant_id, query_text, context_info=None):
    """
    –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –∑–∞–¥–∞—á–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞ –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –±–µ–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –¥–∏–∞–ª–æ–≥
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –±—ã—Å—Ç—Ä—ã—Ö –æ–¥–Ω–æ—Ä–∞–∑–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
    """
    
    app = create_app()
    with app.app_context():
        try:
            assistant = Application.query.get(assistant_id)
            if not assistant:
                return {'status': 'error', 'message': '–ü–æ–º–æ—â–Ω–∏–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω'}
            
            # –ü–æ–ª—É—á–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ FastAPI
            fastapi_url = app.config.get('FASTAPI_URL', 'http://localhost:8002')
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
            search_results = []
            if assistant.enable_search and assistant.status == 'indexed':
                try:
                    search_response = requests.post(f"{fastapi_url}/search", json={
                        "application_id": str(assistant_id),
                        "query": query_text,
                        "limit": 15,  # –ë–æ–ª—å—à–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –æ–¥–Ω–æ—Ä–∞–∑–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
                        "use_reranker": True,
                        "rerank_limit": 30
                    })
                    
                    if search_response.status_code == 200:
                        search_results = search_response.json().get("results", [])
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ: {e}")
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
            context = format_search_context(search_results, max_results=10)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –µ—Å–ª–∏ –µ—Å—Ç—å
            if context_info:
                context = f"–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:\n{context_info}\n\n{context}"
            
            # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏
            support_prompt = """–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç —Å–ª—É–∂–±—ã –ø–æ–¥–¥–µ—Ä–∂–∫–∏. –ü–æ–º–æ–≥–∏ —Ä–µ—à–∏—Ç—å –ø—Ä–æ–±–ª–µ–º—É –∫–ª–∏–µ–Ω—Ç–∞.

–ó–∞–ø—Ä–æ—Å: {query}

–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π:
{context}

–ü—Ä–µ–¥–æ—Å—Ç–∞–≤—å –æ—Ç–≤–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ:

## üìã –ê–Ω–∞–ª–∏–∑
[–ß—Ç–æ –ø–æ–Ω—è–ª –∏–∑ –∑–∞–ø—Ä–æ—Å–∞]

## ‚úÖ –†–µ—à–µ–Ω–∏–µ
[–ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —à–∞–≥–∏]

## ‚ÑπÔ∏è –ü—Ä–∏–º–µ—á–∞–Ω–∏—è
[–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –µ—Å–ª–∏ –Ω—É–∂–Ω–æ]"""
            
            final_prompt = support_prompt.format(
                query=query_text,
                context=context if context else "–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –ø—É—Å—Ç–∞."
            )
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ LLM
            try:
                llm_response = requests.post(
                    f"{fastapi_url}/llm/process",
                    json={
                    "model_name": assistant.llm_model or 'gemma3:27b',
                    "prompt": final_prompt,
                    "context": "",
                    "parameters": {
                        'temperature': 0.3,  # –ù–∏–∑–∫–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏
                        'max_tokens': 2500
                    },
                        "query": query_text
                    },
                    timeout=60
                )
                
                if llm_response.status_code == 200:
                    response_text = llm_response.json().get("response", "–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏")
                else:
                    response_text = "–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ LLM"
                    
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ LLM: {e}")
                response_text = "–°–µ—Ä–≤–∏—Å –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω"
            
            return {
                'status': 'success',
                'response': response_text,
                'sources': [r.get('metadata', {}).get('document_id') for r in search_results[:5]]
            }
            
        except Exception as e:
            logger.exception(f"–û—à–∏–±–∫–∞ –≤ process_support_query: {e}")
            return {'status': 'error', 'message': str(e)}


def _resolve_llm_model_for_message(assistant: Application, template_id: int = None) -> str:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –º–æ–¥–µ–ª—å LLM –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–æ–æ–±—â–µ–Ω–∏—è.
    –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç:
      1) –ï—Å–ª–∏ –≤—ã–±—Ä–∞–Ω —à–∞–±–ª–æ–Ω –∏ —É –Ω–µ–≥–æ –µ—Å—Ç—å —Ç–µ–≥ –≤–∏–¥–∞ llm:MODEL ‚Äî –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –µ–≥–æ
      2) –ï—Å–ª–∏ —É –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –ø—Ä–∏–≤—è–∑–∞–Ω –ø–µ—Ä–≤—ã–π —à–∞–±–ª–æ–Ω —Å —Ç–µ–≥–æ–º llm:MODEL ‚Äî –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –µ–≥–æ
      3) –ò–Ω–∞—á–µ ‚Äî –º–æ–¥–µ–ª—å –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
    """
    try:
        # 1) –Ø–≤–Ω–æ –≤—ã–±—Ä–∞–Ω–Ω—ã–π —à–∞–±–ª–æ–Ω
        if template_id:
            t = Checklist.query.get(template_id)
            if t and t.tags:
                for tag in t.tags:
                    if isinstance(tag, str) and tag.startswith('llm:'):
                        return tag.split(':', 1)[1]
        # 2) –õ—é–±–æ–π –ø—Ä–∏–≤—è–∑–∞–Ω–Ω—ã–π –∫ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç—É —à–∞–±–ª–æ–Ω —Å —Ç–µ–≥–æ–º llm
        if assistant.checklists:
            candidates = assistant.checklists if isinstance(assistant.checklists, list) else assistant.checklists.all()
            for t in candidates:
                if t.tags:
                    for tag in t.tags:
                        if isinstance(tag, str) and tag.startswith('llm:'):
                            return tag.split(':', 1)[1]
    except Exception:
        pass
    # 3) –ú–æ–¥–µ–ª—å –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    return assistant.llm_model or 'gemma3:27b'